# spigot-case-study

### Part 1

The application follows the Android clean architecture model to use a ViewModel to handle communication between the data and the UI.  The Room library is used to create and query from a SQLite table.  Subscribers are set on the data retrieved from the device as well as the input query and the data is updated through the ViewModel when the Parse button is clicked.  The Post button handles adding a new Volley JsonRequest to the RequestQueue, and displays the response in a dialog.  

### Part 2

I spoke a bit about this on the phone with Jacob but one of the core features in the Android application I built for MiC relies on a text recognition feature from an image (specifically a bitmap) captured by the user's device.  Considering that I'm not a machine learning expert and doubt I can develop an AI better than the market leaders, we decided to subscribe to the Google Vision API, which makes a call to the cloud service and waits for a response.  Given the nature of slot machines having lots of "white noise" and mismatched fonts and colors etc many other 3rd party APIs we tested didn't get us to the accuracy levels we required.  We found that Google did a really good job at accurately detecting nearly every alphanumeric character in the image, but it returned them all individually in a map type object with information about the location (x and y coordinates) in the bitmap.  From there we had to develop a system to 1) accurately determine which of the characters are relevant to the values we're attempting to capture, and 2) of those values that are relevant, parse them together to form the numerical value that will eventually be displayed to the user.  

In order to do this we developed a set of heuristics to apply to the response data using the location parameters available to us.  The first step is to iterate through the characters until the first $ character is reached, then keep reading characters and appending to the current value until one is encountered that is deemed to be part of a different "word".  Initially we look at the sizing of the characters and make two assumptions based on sizing.  First we check if the height or width of a character is either more than 2x or less than 1/2x the height or width of it's predecessor.  If either of these conditions are true we can assume we've finished reading the current number and move on to the next one.  The second assumption we make is based on spacing between the characters.  By calculating the difference in width between each pair of characters we calculate a running average width between those characters and if a new character is more than a certain factor away from it's neighbor we can pretty confidently say that we've reached the end of the number.  

This isn't a really complicated system but it is one of the more interesting components of the application.  We have a collection of sample images that we test on so we can see how overall performance changes when tweaks are made.  